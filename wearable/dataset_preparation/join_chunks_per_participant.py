"""
join_chunks_per_participant.py — Joins all Parquet chunk files belonging
to each participant into a single file <pid>_full.parquet.

This module scans the participant output directory (generated by the
participants_pipeline.py), loads all chunk_XXX.parquet files in order,
concatenates them, and writes a single full Parquet file per participant.

Output structure created:
    participants/P001/P001_full.parquet
    participants/P002/P002_full.parquet
    ...

This does NOT perform feature computation, windowing, or annotation steps.
It only merges precomputed chunks.
"""

import pandas as pd
from pathlib import Path

from .config import (
    OUT_DIR,            # directory with participants/
    METADATA_PATH,      # metadata.csv to get list of PIDs
)


def join_chunks(metadata = METADATA_PATH , all_participant_path = OUT_DIR):
    """
    Joins all chunk_*.parquet files for each participant.

    Parameters
    ----------
    metadata : str
        Path to metadata.csv containing a 'pid' column.

    all_participant_path : str
        Base directory where participant subfolders (P001, P002, ...)
        are stored.

    Returns
    -------
    None
        Writes <pid>_full.parquet inside each participant directory.
    """

    metadata_df = pd.read_csv(metadata)
    pid = metadata_df['pid']

    for id in pid:
        participant_path = Path(all_participant_path) / id
        chunk_files = sorted(participant_path.glob("chunk_*.parquet"))

        dfs = []
        for f in chunk_files:
            df = pd.read_parquet(f)
            dfs.append(df)

        full_df=pd.concat(dfs, ignore_index=True)
        output_path= participant_path/f"{id}_full.parquet"
        full_df.to_parquet(output_path, index=False)
        print(f"[OK] {id} chunks successfully joined → {output_path}")
